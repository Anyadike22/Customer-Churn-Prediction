# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAKrY-hNJ4EODfOKFnZJr6irdpYn_DyC
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

df = pd.read_csv('/content/Telco-Customer-Churn.csv')

df.head()

# Preprocessing
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)
df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})

# Splitting the data
X = df.drop('Churn', axis=1)
y = df['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Feature engineering
categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',
                        'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
                        'Contract', 'PaperlessBilling', 'PaymentMethod']
numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']

# Scaling numerical features
scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[numerical_features]), columns=numerical_features)
X_test_scaled = pd.DataFrame(scaler.transform(X_test[numerical_features]), columns=numerical_features)

# One-hot encoding categorical features
encoder = OneHotEncoder(sparse=False)
X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[categorical_features]), columns=encoder.get_feature_names_out(categorical_features))
X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_features]), columns=encoder.get_feature_names_out(categorical_features))

# Combining scaled and encoded features
X_train_prepared = pd.concat([X_train_scaled, X_train_encoded], axis=1)
X_test_prepared = pd.concat([X_test_scaled, X_test_encoded], axis=1)

# Training the models
models = {
    'RandomForest': RandomForestClassifier(random_state=1),
    'ExtraTrees': ExtraTreesClassifier(random_state=1),
    'XGBoost': XGBClassifier(random_state=1),
    'LightGBM': LGBMClassifier(random_state=1)
}

# Fit the models and evaluate
for name, model in models.items():
    model.fit(X_train_prepared, y_train)
    print(f"{name} Test Accuracy: {model.score(X_test_prepared, y_test)}")

#To improve the Extra Trees Classifier, you will use the following parameters (number of estimators, minimum number of samples, minimum number of samples for leaf node and the number of features to consider when looking for the best split) for the hyperparameter grid needed to run a Randomized Cross Validation Search (RandomizedSearchCV).
# n_estimators = [50, 100, 300, 500, 1000]
# min_samples_spli

min_samples_split = [5, 10, 15, 100]
min_samples_leaf = [1, 2, 5, 10]
max_features = ['auto', 'sqrt', 'log2', None]

#Train a new ExtraTreesClassifier Model with the new Hyperparameters from the RandomizedSearchCV (with random_state = 1). Is the accuracy of the new optimal model higher or lower than the initial ExtraTreesClassifier model with no hyperparameter tuning?

# New ExtraTreesClassifier with hyperparameters
new_model = ExtraTreesClassifier(min_samples_split=15, min_samples_leaf=5, max_features='auto', random_state=1)
new_model.fit(X_train_prepared, y_train)
new_accuracy = new_model.score(X_test_prepared, y_test)

# Comparison with initial model
initial_accuracy = models['ExtraTrees'].score(X_test_prepared, y_test)

print(f"Initial ExtraTreesClassifier Accuracy: {initial_accuracy}")
print(f"New ExtraTreesClassifier Accuracy: {new_accuracy}")

# Check if new accuracy is higher or lower
if new_accuracy > initial_accuracy:
    print("The new model with hyperparameter tuning has higher accuracy.")
else:
    print("The new model with hyperparameter tuning has lower accuracy.")



